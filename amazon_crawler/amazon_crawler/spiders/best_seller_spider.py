# spiders/best_seller_spider.py
import random
import json
import time
from datetime import datetime
import asyncio

import scrapy
from scrapy.item import Item, Field
from scrapy.utils.project import get_project_settings
from scrapy_playwright.page import PageMethod


class AmazonProductItem(Item):
    datetime = Field()
    crawl_datetime = Field()
    product_name = Field()
    board_name = Field()
    price = Field()
    ranking = Field()
    review_cnt = Field()
    error = Field()


class bestsellerSpider(scrapy.Spider):
    name = "best"
    allowed_domains = ["amazon.com"]

    # url â†” board_name ë§¤í•‘
    url_board_map = {
        "3015429011": "BEST_External SSD",
        "1292116011": "BEST_Internal SSD",
        "3151491": "BEST_Flash Drive",
        "3015433011": "BEST_Micro SD",
        "1197396": "BEST_SD",
    }

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        settings = get_project_settings()
        self.user_agents = settings.get("USER_AGENT_CHOICES")
        self.headers = settings.get("DEFAULT_REQUEST_HEADERS")

        self.urls = [
            # external ssd / internal ssd / flash drive / micro sd / sd
            "https://www.amazon.com/Best-Sellers-Electronics-External-Solid-State-Drives/zgbs/electronics/3015429011",
            # "https://www.amazon.com/Best-Sellers-Computers-Accessories-Internal-Solid-State-Drives/zgbs/pc/1292116011",
            # "https://www.amazon.com/Best-Sellers-Computers-Accessories-USB-Flash-Drives/zgbs/pc/3151491",
            # "https://www.amazon.com/gp/bestsellers/pc/3015433011",
            # "https://www.amazon.com/Best-Sellers-Computers-Accessories-SecureDigital-Memory-Cards/zgbs/pc/1197396",
        ]

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Requests
    def start_requests(self):
        with open("./config/amazon_cookies.json", "r") as f:
            cookies = json.load(f)
        c = {cookie['name']: cookie['value'] for cookie in cookies}
        self.logger.info("â–¶â–¶â–¶ async start() í˜¸ì¶œë¨")

        for url in self.urls:
            self.logger.info(f"í¬ë¡¤ë§ ì‹œì‘: {url}")
            ua = random.choice(self.user_agents)
            hdrs = self.headers.copy()
            hdrs["User-Agent"] = ua
            self.logger.debug(f"ì‚¬ìš©ëœ User-Agent: {ua}")
            yield scrapy.Request(
                url,
                callback=self.parse,
                errback=self.errback_handler,
                headers=hdrs,
                cookies=c,
                dont_filter=True,
                meta={
                    "playwright": True,
                    "playwright_include_page": True,
                }
            )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Parse
    def parse(self, response):

        """ë² ìŠ¤íŠ¸ì…€ëŸ¬ 50ê°œ ì¹´ë“œ íŒŒì‹±"""
        board_name = self._board_name_from_url(response.url)

        # ëª¨ë“  ê°€ëŠ¥í•œ ì…€ë ‰í„°ë¡œ ì¹´ë“œ ì°¾ê¸°
        cards = response.xpath('//div[@id="gridItemRoot"]')
        self.logger.info(f"{board_name}: {len(cards)}ê°œ ì¹´ë“œ ê°ì§€")

        # 30ê°œë§Œ ë‚˜ì˜¤ëŠ” ê²½ìš° ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        if len(cards) <= 30:
            self.logger.warning(f"âš ï¸  ì˜ˆìƒë³´ë‹¤ ì ì€ ì¹´ë“œ ìˆ˜: {len(cards)}")
            
            # í˜ì´ì§€ ì†ŒìŠ¤ ì¼ë¶€ í™•ì¸
            self.logger.info("HTML êµ¬ì¡° í™•ì¸ì„ ìœ„í•œ ìƒ˜í”Œ:")
            sample_html = response.text[:2000]  # ì²˜ìŒ 2000ìë§Œ
            self.logger.info(f"HTML ìƒ˜í”Œ: {sample_html}")

        for i, card in enumerate(cards, 1):
            item = AmazonProductItem()
            item["board_name"] = board_name
            item["crawl_datetime"] = datetime.now().strftime("%Y/%m/%d %H:%M:%S")

            # â”€â”€â”€ ë­í‚¹ (#1, #2 â€¦)
            rank_selectors = [
                './/span[@class="zg-bdg-text"]/text()',
                './/span[contains(@class, "zg-bdg-text")]/text()',
                './/span[contains(@class, "zg-badge-text")]/text()'
            ]
            
            rank = None
            for selector in rank_selectors:
                rank = card.xpath(selector).get()
                if rank:
                    break
            item["ranking"] = rank.strip() if rank else f"#{i}"

            # â”€â”€â”€ ì œí’ˆëª…
            title_selectors = [
                './/div[contains(@class,"p13n-sc-css-line-clamp-3")]/text()',
                './/h3//text()',
                './/a[contains(@class, "a-link-normal")]//text()',
                './/span[contains(@class, "a-size-mini")]//text()',
                './/div[contains(@class, "a-section")]//text()'
            ]
            
            title = None
            for selector in title_selectors:
                title = card.xpath(selector).get()
                if title and title.strip():
                    break
            item["product_name"] = title.strip() if title else f"Product_{i}"

            # â”€â”€â”€ ë¦¬ë·° ìˆ˜
            review_selectors = [
                './/a[contains(@href,"product-reviews")]/span/text()',
                './/span[contains(@class, "a-size-small")]//text()',
                './/a[contains(@href, "product-reviews")]//text()'
            ]
            
            reviews = None
            for selector in review_selectors:
                reviews = card.xpath(selector).get()
                if reviews and any(char.isdigit() for char in reviews):
                    break
            
            if reviews:
                # ìˆ«ìë§Œ ì¶”ì¶œ
                import re
                numbers = re.findall(r'[\d,]+', reviews)
                reviews = numbers[0] if numbers else "0"
            item["review_cnt"] = reviews.replace(",", "") if reviews else "0"

            # â”€â”€â”€ ê°€ê²©
            price_selectors = [
                './/span[contains(@class,"p13n-sc-price")]/text()',
                './/span[@class="a-price-whole"]/text()',
                './/span[contains(@class, "a-price")]//text()',
                './/span[contains(@class, "price")]//text()'
            ]
            
            price = None
            for selector in price_selectors:
                price = card.xpath(selector).get()
                if price and ('$' in price or any(char.isdigit() for char in price)):
                    break
            item["price"] = price.strip() if price else "null"

            # ì§„í–‰ìƒí™© ë¡œê·¸ (ì²˜ìŒ 5ê°œì™€ ë§ˆì§€ë§‰ 5ê°œë§Œ)
            if i <= 5 or i > len(cards) - 5:
                self.logger.info(f"#{i}: {item['product_name'][:30]}... | ê°€ê²©: {item['price']} | ë¦¬ë·°: {item['review_cnt']}")

            yield item

        # ìµœì¢… ê²°ê³¼ ë¡œê·¸
        self.logger.info(f"ğŸ¯ {board_name} í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(cards)}ê°œ ìƒí’ˆ ìˆ˜ì§‘")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Errback
    def errback_handler(self, failure):
        request = failure.request
        item = AmazonProductItem()
        item["crawl_datetime"] = datetime.now().strftime("%Y/%m/%d %H:%M:%S")
        item["board_name"] = self._board_name_from_url(request.url)
        item["error"] = f"ìš”ì²­ ì‹¤íŒ¨: {failure.value}"
        self.logger.error(f"[Errback] {request.url} â†’ {failure.value}")
        yield item

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Helper
    def _board_name_from_url(self, url: str) -> str:
        """url ì† ì¹´í…Œê³ ë¦¬ ì½”ë“œë¡œ board_name ì¶”ì¶œ"""
        for code, name in self.url_board_map.items():
            if code in url:
                return name
        return "unknown"


if __name__ == "__main__":
    from scrapy.crawler import CrawlerProcess
    from scrapy.utils.project import get_project_settings

    settings = get_project_settings()
    process = CrawlerProcess(settings)
    process.crawl(bestsellerSpider)
    process.start()